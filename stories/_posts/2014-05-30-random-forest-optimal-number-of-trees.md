---
title: "Чи залежить оптимальна кількість дерев у алгоритмі Random Forest від кількості предикторів?"
author: Девіняк Олег
tags: [Random Forest]
date: "2014-05-30"
layout: post
category: myblog
---

Є [один крутий сайт](http://stats.stackexchange.com) де можна задати питання зі статистики (англійською), і якщо питання адекватне, то вам обов'язково дадуть відповідь. Відповіді дають за просто так, щоб допомогти. Максимум користі для відповідача - те, що відповіді фіксуються і оцінюються іншими спеціалістами, у зв'язку з чим профіль на сайті може бути хорошою візитною карткою фахівця. Зрідка і я давав відповіді на питання відвідувачів. І моя [відповідь на питання, яке винесене в заголовок](http://stats.stackexchange.com/questions/36165/does-the-optimal-number-of-trees-in-a-random-forest-depend-on-the-number-of-pred/36183#36183), набрала найбільше балів (не на всьому сайті, тільки серед інших моїх відповідей). 15 спеціалістів оцінили її як корисну, а кількість переглядів досягла майже 4 тис (за цю відповідь навіть отримав значок **nice answer** - не можу з нього натішитись:). Думаю це є достатньою підставою, щоб перекласти відповідь українською. Але якщо ви не знаєте, що таке алгоритм Random Forest - краще далі не читайте.

Отже, питання були такі:

>Чи залежить оптимальна кількість дерев у алгоритмі Random Forest від кількості предикторів? Чи може хтось пояснити, чому треба використовувати велику кількість дерев у Random Forest коли кількість предикторів значна? І як визначити оптимальну кількість дерев?

А тепер процитую свою відповідь:

> Random Forest використовує **беггінг** (*bagging* - використання вибірки із спостережень замість всіх спостережень) і **метод випадкових підпросторів** (*random subspace method* - використання вибірки із предикторів замість всіх предикторів, іншими словами - беггінг ознак) щоб виростити дерево. Якщо кількість спостережень є значною, але кількість дерев занадто мала, тоді деякі спостереження будуть спрогнозовані лише один раз, або й навіть жодного разу. Якщо кількість предикторів є значною, а кількість дерев занадто мала, тоді деякі предиктори можуть (теоретично) бути пропущеними у всіх використовуваних підпросторах. Обидва випадки призводять до зменшення прогностичної здатності. Але останній є дуже рідкісним випадком, оскільки вибір підпростору виконується у кожному вузлі. При класифікації розмірність підростору по замовчуванню складає \\(\sqrt{р}\\) (досить мала, *р* є загальною кількістю предикторів), однак дерево містить багато вузлів. При регресії розмірність підпростору по замовчуванню складає \\(p/3\\) (досить велика), але дерево містить менше вузлів. Отже, оптимальна кількість дерев у Random Forest залежить від кількості предикторів лише в екстремальних випадках. [Офіційна сторінка алгоритму](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks) зазначає, що Random Forest не має надмірної підгонки (перетренування, *overfit*), і ви можете використовувати стільки дерев, скільки душа забажає. Але <a href="#segal>Mark E. Segal</a> повідомив, що надмірна підгонка трапляється для зашумлених даних. Тому, щоб знайти оптимальну кількість дерев, можна будувати Random Forest при послідовності значеннь параметра *ntree* (це просто, але більш затратно по об'єму обчислень на процесорі), або ж побудувати один Random Forest з великою кількістю дерев та параметром *keep.inbag*, обчислити частоти похибок *OOB* для перших *n* дерев (де *n* змінюється від 1 до *ntree*) і вивести графік залежності частоти похибок *OOB* від кількості дерев (це більш складно, але вимагає значно менше процесорного часу).

___

<div class="nohover">
<a name="segal", id="anchor">Mark E. Segal Machine Learning Benchmarks and Random Forest Regression. April 14 2004. Center for Bioinformatics & Molecular Biostatistics</a>
</div>




